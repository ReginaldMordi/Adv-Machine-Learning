{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "R NoteBook",
      "provenance": [],
      "authorship_tag": "ABX9TyPP9s/EhXRQF87QYZnLaLo+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ReginaldMordi/Adv-Machine-Learning/blob/main/R_NoteBook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ATcFVwDocs0"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8Q7YY8rpF2g"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_fyZ3yXaCUC",
        "outputId": "7922f613-dc6e-4776-8c4d-57d7bcf73f6f"
      },
      "source": [
        "library(ggplot2)\r\n",
        "library(tidyverse)\r\n",
        "install.packages(\"cowplot\")\r\n",
        "install.packages(\"keras\")\r\n",
        "#devtools::install_github(\"rstudio/tensorflow\")\r\n",
        "library(tensorflow)\r\n",
        "library(cowplot)\r\n",
        "library(keras)\r\n",
        "\r\n",
        "#install_tensorflow()\r\n",
        "# Importing IMDB movie reviews dataset and we only keep the top 10,000 most frequently occurring words in the training data.\r\n",
        "imdb <- dataset_imdb(num_words = 10000)\r\n",
        "c(c(train_data, train_labels), c(test_data, test_labels)) %<-% imdb"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjhJTkqKa5kZ",
        "outputId": "dcb9424c-c755-4494-e863-3b4f54820111"
      },
      "source": [
        "\r\n",
        "# Converting my data to binary data\r\n",
        "vectorize_sequences <- function(sequences, dimention = 10000) { \r\n",
        "    # Create an all-zero matrix of shape (len(sequences), dimension)\r\n",
        "  results <- matrix(0, nrow = length(sequences), ncol= dimention)\r\n",
        "  for(i in 1:length(sequences))\r\n",
        "        # Sets specific indices of results[i] to 1s\r\n",
        "  results[i, sequences[[i]]] <- 1\r\n",
        "  results\r\n",
        "}\r\n",
        "\r\n",
        "x_train <- vectorize_sequences(train_data)  # vectorized training data\r\n",
        "x_test <- vectorize_sequences(test_data)    # vectorized test data\r\n",
        "# vectoring the labels\r\n",
        "y_train <- as.numeric(train_labels)\r\n",
        "y_test <- as.numeric(test_labels)\r\n",
        "# structure of the vectorized samples\r\n",
        "str(x_train[1,])\r\n",
        "\r\n",
        "# Validating my approach\r\n",
        "## Setting apart 10,000 samples from the original training data for validation\r\n",
        "\r\n",
        "val_indices <- 1:10000\r\n",
        "x_val <- x_train[val_indices,]\r\n",
        "partial_x_train <- x_train[-val_indices,]\r\n",
        "y_val <- y_train[val_indices]\r\n",
        "partial_y_train <- y_train[-val_indices]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " num [1:10000] 1 1 0 1 1 1 1 1 1 0 ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6NUJUzebFyx"
      },
      "source": [
        "model1 <- keras_model_sequential() %>% \r\n",
        "  layer_dense(units = 16, activation = \"relu\", input_shape = c(10000)) %>% \r\n",
        "  layer_dense(units = 16, activation = \"relu\") %>% \r\n",
        "  layer_dense(units = 1, activation = \"sigmoid\")\r\n",
        "model1 %>% compile(\r\n",
        "  optimizer = \"rmsprop\",\r\n",
        "  loss = \"binary_crossentropy\",\r\n",
        "  metrics = c(\"accuracy\"))\r\n",
        "history <- model1 %>% fit(\r\n",
        "  partial_x_train,\r\n",
        "  partial_y_train,\r\n",
        "  epochs = 20,\r\n",
        "  batch_size = 512,\r\n",
        "  validation_data = list(x_val, y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arIT__5ubjr3"
      },
      "source": [
        "# Building our network \r\n",
        "# Using 3 hidden layers with 32, 32, 32 units, tanh activation function, batch size 512, 20 epoch.\r\n",
        "\r\n",
        "model333 <- keras_model_sequential() %>% \r\n",
        "  layer_dense(units = 32, activation = \"tanh\", input_shape = c(10000)) %>%\r\n",
        "  layer_dense(units = 32, activation = \"tanh\") %>%\r\n",
        "  layer_dense(units = 32, activation = \"tanh\") %>%\r\n",
        "  layer_dense(units = 1, activation = \"sigmoid\")\r\n",
        "model333 %>% compile(\r\n",
        "  optimizer = \"rmsprop\",\r\n",
        "  loss = \"mse\",\r\n",
        "  metrics = c(\"accuracy\"))\r\n",
        "history_3_layer32 <- model333 %>% fit(\r\n",
        "  partial_x_train,\r\n",
        "  partial_y_train,\r\n",
        "  epochs = 20,\r\n",
        "  batch_size = 512,\r\n",
        "  validation_data = list(x_val, y_val))\r\n",
        "\r\n",
        "# Let's train a new network from scratch for 4 epochs and then evaluate it on the test data.\r\n",
        "\r\n",
        "model3331 <- keras_model_sequential() %>% \r\n",
        "  layer_dense(units = 32, activation = \"tanh\", input_shape = c(10000)) %>%\r\n",
        "  layer_dense(units = 32, activation = \"tanh\") %>%\r\n",
        "  layer_dense(units = 32, activation = \"tanh\") %>%\r\n",
        "  layer_dense(units = 1, activation = \"sigmoid\")\r\n",
        "\r\n",
        "model3331 %>% compile(\r\n",
        "  optimizer = \"rmsprop\",\r\n",
        "  loss = \"mse\",\r\n",
        "  metrics = c(\"accuracy\"))\r\n",
        "\r\n",
        "model3331 %>% fit(x_train, y_train, epochs = 5, batch_size = 512)\r\n",
        "results1 <- model3331 %>% evaluate(x_test, y_test)\r\n",
        "results1\r\n",
        "\r\n",
        "# Visualizing the model331 output of loss function and accuracy\r\n",
        "model331.df <- as.data.frame(history_3_layer32$metrics)\r\n",
        "names(model331.df) <- c(\"train_loss\",\"train_accuracy\",\"val_loss\",\"val_accuracy\")\r\n",
        "model331.df <- model331.df %>% mutate(epochs=1:n()) %>% gather(\"split\",\"values\",-epochs) %>% separate(split,c(\"split\",\"metric\")) %>% spread(metric,values)\r\n",
        "p1<-ggplot(model331.df) + geom_line(aes(x=epochs,y=loss,color=split),size=0.8)+geom_point(aes(x=epochs,y=loss,color=factor(split)),size=1.5)+ggtitle(\"Epochs vs Loss function  with 3 hidden layers(32,32,32)\")+theme(panel.grid = element_blank(),panel.background = element_blank())+theme_classic()+theme(legend.position = 'top',legend.justification = 'left',legend.title = element_blank())\r\n",
        "p2<-ggplot(model331.df) + geom_line(aes(x=epochs,y=accuracy,color=split),size=0.8,show.legend = F)+geom_point(aes(x=epochs,y=accuracy,color=split),size=1.5,show.legend = F)+ggtitle(\"Epochs vs Accuracy\")+theme(panel.grid = element_blank(),panel.background = element_blank())+theme_classic()\r\n",
        "plot_grid(p1,p2,nrow = 2)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tM0_d5ArZo8J"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "412ew9UEdTod"
      },
      "source": [
        "\r\n",
        "# Using 3 hidden layers with 64, 32, 16 units, tanh activation function, batch size 512, 20 epoch.\r\n",
        "model632 <- keras_model_sequential() %>% \r\n",
        "  layer_dense(units = 64,kernel_regularizer = regularizer_l1(0.001), activation = \"tanh\",input_shape = c(10000)) %>%\r\n",
        "  layer_dropout(rate = 0.5) %>%\r\n",
        "  layer_dense(units = 32,kernel_regularizer = regularizer_l1(0.001), activation = \"tanh\") %>%\r\n",
        "  layer_dropout(rate = 0.5) %>%\r\n",
        "  layer_dense(units = 16,kernel_regularizer = regularizer_l1(0.001), activation = \"tanh\") %>%\r\n",
        "  layer_dropout(rate = 0.5) %>%\r\n",
        "  layer_dense(units = 1, activation = \"sigmoid\")\r\n",
        "model632 %>% compile(\r\n",
        "  optimizer = \"rmsprop\",\r\n",
        "  loss = \"mse\",\r\n",
        "  metrics = c(\"accuracy\"))\r\n",
        "history_3 <- model632 %>% fit(\r\n",
        "  partial_x_train,\r\n",
        "  partial_y_train,\r\n",
        "  epochs = 20,\r\n",
        "  batch_size = 512,\r\n",
        "  validation_data = list(x_val, y_val))\r\n",
        "\r\n",
        "# Training a new network from scratch for 4 epochs and then evaluate it on the test data.\r\n",
        "\r\n",
        "model6313 <- keras_model_sequential() %>% \r\n",
        "  layer_dense(units = 64, activation = \"tanh\",input_shape = c(10000)) %>%\r\n",
        "  layer_dropout(rate = 0.5) %>%\r\n",
        "  layer_dense(units = 32, activation = \"tanh\") %>%\r\n",
        "  layer_dropout(rate = 0.5) %>%\r\n",
        "  layer_dense(units = 16, activation = \"tanh\") %>%\r\n",
        "  layer_dropout(rate = 0.5) %>%\r\n",
        "  layer_dense(units = 1, activation = \"sigmoid\")\r\n",
        "model6313 %>% compile(\r\n",
        "  optimizer = \"rmsprop\",\r\n",
        "  loss = \"mse\",\r\n",
        "  metrics = c(\"accuracy\"))\r\n",
        "model6313 %>% fit(x_train, y_train, epochs = 4, batch_size = 512)\r\n",
        "results4 <- model6313 %>% evaluate(x_test, y_test)\r\n",
        "results4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNLTk4AvZq5A"
      },
      "source": [
        "# Visualizing the model6313 output of loss function and accuracy\r\n",
        "model6313.df <- as.data.frame(history_3$metrics)\r\n",
        "names(model6313.df) <- c(\"train_loss\",\"train_accuracy\",\"val_loss\",\"val_accuracy\")\r\n",
        "model6313.df <- model6313.df %>% mutate(epochs=1:n()) %>% gather(\"split\",\"values\",-epochs) %>% separate(split,c(\"split\",\"metric\")) %>% spread(metric,values)\r\n",
        "p1<-ggplot(model6313.df) + geom_line(aes(x=epochs,y=loss,color=split),size=0.8)+geom_point(aes(x=epochs,y=loss,color=factor(split)),size=1.5)+ggtitle(\"Epochs vs Loss function  with 3 hidden layers(32,32,32)\")+theme(panel.grid = element_blank(),panel.background = element_blank())+theme_classic()+theme(legend.position = 'top',legend.justification = 'left',legend.title = element_blank())\r\n",
        "p2<-ggplot(model6313.df) + geom_line(aes(x=epochs,y=accuracy,color=split),size=0.8,show.legend = F)+geom_point(aes(x=epochs,y=accuracy,color=split),size=1.5,show.legend = F)+ggtitle(\"Epochs vs Accuracy\")+theme(panel.grid = element_blank(),panel.background = element_blank())+theme_classic()\r\n",
        "plot_grid(p1,p2,nrow = 2)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}